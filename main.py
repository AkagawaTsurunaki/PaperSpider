import json
import os.path
import time

import pandas as pd
from loguru import logger
from tqdm import tqdm

from config import read_config
from parser.detail_parser import parse_detail, fetch_detail_page
from parser.search_parser import fetch_search_page, parse_search_page
from util import read_html, save_html, to_valid_filename, remove_html

_config = read_config()
_database = {}
_result_json_path = "result.json"
_excel_output_path = "result.xlsx"


def analyze_base_info():
    # ç¬¬ä¸€é˜¶æ®µå…ˆè·å–åŸºç¡€ä¿¡æ¯
    for searchissn in tqdm(set(_config.issnList), desc="åˆ†æè®ºæ–‡åŸºç¡€ä¿¡æ¯"):
        html_doc = None
        if not _config.overwriteExistedHtml:
            try:
                html_doc = read_html(to_valid_filename(searchissn))
            except FileNotFoundError:
                pass
        if html_doc is None:
            html_doc = fetch_search_page(searchissn=searchissn)
            save_html(to_valid_filename(searchissn), html_doc)
            time.sleep(_config.sleepInterval)
        result = parse_search_page(html_doc)
        if result['journalid'] is None:
            logger.warning(f"ğŸ˜­ ISSN ä¸º {searchissn} çš„å†…å®¹æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ ISSN åé‡è¯•ï¼šjournalid ä¸º null")
            remove_html(to_valid_filename(searchissn))

        _database[result['journalid']] = result

    for searchname in tqdm(set(_config.nameList)):
        html_doc = None
        if not _config.overwriteExistedHtml:
            try:
                html_doc = read_html(to_valid_filename(searchname))
            except FileNotFoundError:
                pass
        if html_doc is None:
            html_doc = fetch_search_page(searchname=searchname)
            save_html(to_valid_filename(searchname), html_doc)
            time.sleep(_config.sleepInterval)
        result = parse_search_page(html_doc)
        if result['journalid'] is None:
            logger.warning(f"ğŸ˜­ æœŸåˆŠ/ä¼šè®®åä¸º {searchname} çš„å†…å®¹æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥æ‚¨è¾“å…¥çš„åç§°åé‡è¯•ï¼šjournalid ä¸º null")
            remove_html(to_valid_filename(searchname))
        _database[result['journalid']] = result

    logger.info("âœ…ï¸ åŸºç¡€ä¿¡æ¯è·å–å®Œæ¯•ï¼")


def analyze_detail_info():
    # ç»†ç²’åº¦åœ°è·å–å†å¹´æŒ‡æ ‡
    for journalid, result in tqdm(_database.items(), desc="åˆ†æè®ºæ–‡è¯¦ç»†ä¿¡æ¯"):
        html_doc = None
        if not _config.overwriteExistedHtml:
            try:
                html_doc = read_html(to_valid_filename(f"detail-{journalid}"))
            except FileNotFoundError:
                pass
        if html_doc is None:
            html_doc = fetch_detail_page(journalid)
            save_html(to_valid_filename(f"detail-{journalid}"), html_doc)
            time.sleep(_config.sleepInterval)

        detail = parse_detail(html_doc)
        for key, value in detail.items():
            result[key] = value
        _database[journalid] = result

    logger.info("âœ…ï¸ è¯¦ç»†ä¿¡æ¯è·å–å®Œæ¯•ï¼")


def save_data():
    with open(_result_json_path, mode='w+', encoding='utf-8') as file:
        file.write(json.dumps(_database, indent=4, ensure_ascii=False))
    logger.info(f"ğŸ’¾ è§£æçš„ JSON æ•°æ®å·²ä¿å­˜è‡³ï¼š{os.path.abspath(_result_json_path)}")


def json_to_excel():
    with open(_result_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    records = [
        {"journalid": jid, **info}
        for jid, info in data.items()
    ]

    df = pd.DataFrame(records)
    df.to_excel(_excel_output_path, index=False)
    logger.info(f"ğŸ’¾ è½¬æ¢çš„ Excel æ–‡ä»¶å·²ä¿å­˜è‡³: {os.path.abspath(_excel_output_path)}")


if __name__ == "__main__":
    try:
        analyze_base_info()
        analyze_detail_info()
        save_data()
        json_to_excel()
        logger.info("ğŸ“‘ PaperSpider è¿è¡Œå®Œæ¯•ï¼")
    except Exception as e:
        logger.exception(e)
        logger.error("âŒ PaperSpider è¿è¡Œå¤±è´¥ï¼è¯·æŸ¥çœ‹é”™è¯¯åŸå› ã€‚")
